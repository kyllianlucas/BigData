# ğŸ“Š Projet Big Data â€“ Ingestion de donnÃ©es via API (Airflow & Docker)

## ğŸ“ Contexte acadÃ©mique

Ce projet a Ã©tÃ© rÃ©alisÃ© dans le cadre dâ€™un **projet Big Data scolaire** avec Ouissal MASBAHI.  
Il vise Ã  mettre en pratique les notions suivantes :

- Consommation dâ€™une **API externe**
- Ingestion de **donnÃ©es brutes**
- Automatisation via **Apache Airflow**
- Conteneurisation avec **Docker**
- GÃ©nÃ©ration et stockage de donnÃ©es au format **CSV**

---

## ğŸ¯ Objectif du projet

Les objectifs principaux sont :

- Appeler une API publique (**data.gouv.fr**)
- RÃ©cupÃ©rer des donnÃ©es tabulaires brutes
- GÃ©nÃ©rer un fichier CSV contenant **exactement 100 lignes**
- Stocker ce fichier dans un dossier `data`
- Ne rÃ©aliser **aucune transformation** des donnÃ©es aprÃ¨s ingestion

---

## ğŸ“¥ Cloner le dÃ©pÃ´t Git

### 1ï¸âƒ£ Ouvrir un terminal

- Windows : CMD, PowerShell ou terminal VS Code  
- macOS / Linux : Terminal

### 2ï¸âƒ£ Se placer dans le dossier de travail

```bash
cd chemin/vers/votre/dossier

```

### 3ï¸âƒ£ Cloner le git

```bash

git clone https://github.com/kyllianlucas/BigData.git

cd BigData

```
ğŸ—‚ï¸ Structure du projet

![alt text](image/structure.png)

ğŸ“„ Description des fichiers

get_api.py :
Script Python chargÃ© dâ€™appeler lâ€™API data.gouv.fr et de rÃ©cupÃ©rer les donnÃ©es.

dag_hebdo.py :
DAG Apache Airflow permettant dâ€™automatiser lâ€™exÃ©cution du script de rÃ©cupÃ©ration.

docker-compose.yaml :
Fichier de configuration Docker permettant de dÃ©ployer Airflow.

data/ :
Dossier de sortie contenant les donnÃ©es brutes.

data_raw_100.csv :
Fichier CSV gÃ©nÃ©rÃ© automatiquement contenant 100 lignes de donnÃ©es brutes.

âš™ï¸ PrÃ©requis
Logiciels requis

Git

Python 3.x

Docker

Docker Compose

Modules Python

requests

Installation du module Python :

```bash
pip install requests

```

ExÃ©cution sans Docker (Python uniquement)

```bash
cd airflow-docker/dags

python dag_hebdo.py

```

ğŸ³ ExÃ©cution avec Docker & Airflow

```bash

cd airflow-docker

docker-compose up -d
```

AccÃ©der Ã  lâ€™interface Airflow

Ouvrir un navigateur :

http://localhost:8080

dentifiants par dÃ©faut :

Utilisateur : airflow

Mot de passe : airflow

4ï¸âƒ£ Activer le DAG

Activer le DAG dag_hebdo

Lancer une exÃ©cution manuelle si nÃ©cessaire

âœ… RÃ©sultat attendu

AprÃ¨s exÃ©cution :

Un fichier est gÃ©nÃ©rÃ© automatiquement :

data/data_raw_100.csv


Le fichier contient exactement 100 lignes

Les donnÃ©es sont issues de lâ€™API data.gouv.fr

Les donnÃ©es sont brutes, non transformÃ©es

ğŸŒ API utilisÃ©e

API tabulaire officielle de data.gouv.fr :

https://tabular-api.data.gouv.fr/api/resources/1c5075ec-7ce1-49cb-ab89-94f507812daf/data/

ğŸ–¼ï¸ Illustration

![alt text](image/dag.png)

Acces a spark et hadoop

```bash
docker exec -it hadoop-master bash 

docker start hadoop-master 
docker start hadoop-worker1 
docker start hadoop-worker2 
AccÃ©der au conteneur master :  
docker exec -it hadoop-master bash 
VÃ©rifier que Spark est installÃ© : spark-shell --version 
Lancer Spark :spark-shell 

```

en cas de non lancement de spark faire les commandes qui suivent en recommanÃ§ant la manipulation precedante sauf la derniere commande ou il faut les passez juste avant

```bash
start-dfs.sh
start-yarn.sh
```

Rapport

### 1ï¸âƒ£ Afficher toutes les donnÃ©es des 5 premiÃ¨res lignes
![alt text](image/rapport1.png)

### 2ï¸âƒ£ VÃ©rifiacations du nombres de lignes dans le fichier

![alt text](image/rapport2.png)

### 3ï¸âƒ£ VÃ©rifications des lignes vides

![alt text](image/rapport3.png)

### 4ï¸âƒ£ VÃ©rifications des doublons

![alt text](image/rapport4.png)

### 5ï¸âƒ£ On ne peux pas normaliser car a la creation du fichier tous tiens sur une ligne 

![alt text](image/rapport5.png)

ğŸ§  Conclusion

Ce projet permet de dÃ©montrer :

La capacitÃ© Ã  consommer une API publique

Lâ€™ingestion de donnÃ©es brutes en Big Data

Lâ€™automatisation avec Apache Airflow

Lâ€™utilisation de Docker pour des environnements reproductibles

Il constitue une base solide pour des traitements Big Data plus avancÃ©s.
