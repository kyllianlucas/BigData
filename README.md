# ğŸ“Š Projet Big Data â€“ Ingestion de donnÃ©es via API (Airflow & Docker)

## ğŸ“ Contexte acadÃ©mique

Ce projet a Ã©tÃ© rÃ©alisÃ© dans le cadre dâ€™un **projet Big Data scolaire**.  
Il vise Ã  mettre en pratique les notions suivantes :

- Consommation dâ€™une **API externe**
- Ingestion de **donnÃ©es brutes**
- Automatisation via **Apache Airflow**
- Conteneurisation avec **Docker**
- GÃ©nÃ©ration et stockage de donnÃ©es au format **CSV**

---

## ğŸ¯ Objectif du projet

Les objectifs principaux sont :

- Appeler une API publique (**data.gouv.fr**)
- RÃ©cupÃ©rer des donnÃ©es tabulaires brutes
- GÃ©nÃ©rer un fichier CSV contenant **exactement 100 lignes**
- Stocker ce fichier dans un dossier `data`
- Ne rÃ©aliser **aucune transformation** des donnÃ©es aprÃ¨s ingestion

---

## ğŸ“¥ Cloner le dÃ©pÃ´t Git

### 1ï¸âƒ£ Ouvrir un terminal

- Windows : CMD, PowerShell ou terminal VS Code  
- macOS / Linux : Terminal

### 2ï¸âƒ£ Se placer dans le dossier de travail

```bash
cd chemin/vers/votre/dossier

git clone https://github.com/kyllianlucas/BigData.git

cd BigData

```
ğŸ—‚ï¸ Structure du projet
BIGDATA/
â”œâ”€â”€ airflow-docker/
â”‚   â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ dag_hebdo.py
â”‚   â”‚   â””â”€â”€ get_api.py
â”‚   â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ plugins/
â”‚   â”œâ”€â”€ .env
â”‚   â””â”€â”€ docker-compose.yaml
â”œâ”€â”€ data/
â”‚   â””â”€â”€ data_raw_100.csv
â”œâ”€â”€ .gitignore
â”œâ”€â”€ image.png
â””â”€â”€ README.md

ğŸ“„ Description des fichiers

get_api.py
Script Python chargÃ© dâ€™appeler lâ€™API data.gouv.fr et de rÃ©cupÃ©rer les donnÃ©es.

dag_hebdo.py
DAG Apache Airflow permettant dâ€™automatiser lâ€™exÃ©cution du script de rÃ©cupÃ©ration.

docker-compose.yaml
Fichier de configuration Docker permettant de dÃ©ployer Airflow.

data/
Dossier de sortie contenant les donnÃ©es brutes.

data_raw_100.csv
Fichier CSV gÃ©nÃ©rÃ© automatiquement contenant 100 lignes de donnÃ©es brutes.

âš™ï¸ PrÃ©requis
Logiciels requis

Git

Python 3.x

Docker

Docker Compose

Modules Python

requests

Installation du module Python :

```bash
pip install requests

```

ExÃ©cution sans Docker (Python uniquement)

```bash
cd airflow-docker/dags

python dag_hebdo.py

```

ğŸ³ ExÃ©cution avec Docker & Airflow

```bash

cd airflow-docker

docker-compose up -d
```

AccÃ©der Ã  lâ€™interface Airflow

Ouvrir un navigateur :

http://localhost:8080

dentifiants par dÃ©faut :

Utilisateur : airflow

Mot de passe : airflow

4ï¸âƒ£ Activer le DAG

Activer le DAG dag_hebdo

Lancer une exÃ©cution manuelle si nÃ©cessaire

âœ… RÃ©sultat attendu

AprÃ¨s exÃ©cution :

Un fichier est gÃ©nÃ©rÃ© automatiquement :

data/data_raw_100.csv


Le fichier contient exactement 100 lignes

Les donnÃ©es sont issues de lâ€™API data.gouv.fr

Les donnÃ©es sont brutes, non transformÃ©es

ğŸŒ API utilisÃ©e

API tabulaire officielle de data.gouv.fr :

https://tabular-api.data.gouv.fr/api/resources/1c5075ec-7ce1-49cb-ab89-94f507812daf/data/

ğŸ–¼ï¸ Illustration

![alt text](image.png)

Acces a spark et hadoop

```bash
docker exec -it hadoop-master bash 

docker start hadoop-master 
docker start hadoop-worker1 
docker start hadoop-worker2 
AccÃ©der au conteneur master :  
docker exec -it hadoop-master bash 
VÃ©rifier que Spark est installÃ© : spark-shell --version 
Lancer Spark :spark-shell 

```

Rapport


![alt text](image-1.png)

![alt text](image-2.png)

![alt text](image-3.png)

![alt text](image-4.png)

on ne peux pas normaliser car a la creation du fichier tous tiens sur une ligne 

![alt text](image-5.png)

ğŸ§  Conclusion

Ce projet permet de dÃ©montrer :

La capacitÃ© Ã  consommer une API publique

Lâ€™ingestion de donnÃ©es brutes en Big Data

Lâ€™automatisation avec Apache Airflow

Lâ€™utilisation de Docker pour des environnements reproductibles

Il constitue une base solide pour des traitements Big Data plus avancÃ©s.

